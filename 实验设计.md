# 实验设计

## A. 压缩元数据实验（目标：证明压缩率、encode/decode 成本、正确性、并发/生命周期管理）

### A1 — 压缩比与序列化开销（micro-benchmark）

**目的**：验证各类 token 的真实字节大小、压缩比，以及序列化/反序列化（encode/decode）开销。
 **假设**：压缩后平均 token 大小接近设计值（PD=8B, MR≈12B, CQ≈12B, QP≈16B）；encode/decode 成本远小于 RDMA 操作延迟量级。
 **平台**：任意 x86 Linux 开发机。
 **软件**：C/C++ 测试程序（参考你给出的 `rdma_token_compress_test.cpp`），编译 `-O3`。
 **工作负载**：生成 N 个对象（N ∈ {1e3, 1e4, 1e5, 1e6}），随机与典型字段分布（small/large页数、access_flags 分布）。
 **步骤**：

1. 实现 `encode(token_struct -> bytes)` 与 `decode(bytes -> token_struct)`，保证对齐/packed。
2. 对每个 N 重复 10 次：测量总编码时间、总解码时间、并输出平均/95th。
3. 记录总字节数、压缩比 = 原始结构字节（估算）/ 实际 bytes。
    **示例测量**：

```
./rdma_token_compress_test --type=QP --N=1000000 --iters=10
# 输出：avg encode us/op, avg decode us/op, total bytes
```

**指标**：avg encode latency (ns/us), avg decode latency, bytes per token, CPU cycles/op。
 **分析**：画 bar chart（token type vs avg size），CDF（encode/decode latency），并计算压缩比随 N 是否稳定。

------

### A2 — Encode/Decode 对主机 CPU 影响（多线程与缓存局部性）

**目的**：评估并发 encode/decode 在多核条件下的可扩展性与缓存压力。
 **假设**：encode/decode 为内存与算术轻量操作，能线性扩展到一定核数后受内存带宽/缓存影响。
 **步骤**：

- 在有 16/32 核的机器，运行多线程 encode/decode（threads ∈ {1,2,4,8,16,32}），每线程处理 M=1e5 tokens。
- 使用 `perf stat`（或 `numactl`）记录 CPU cycles、LLC misses、instructions。
   **指标**：throughput (tokens/sec) vs threads；per-thread latency；LLC miss rate。
   **分析**：展示 scalability 曲线，找出瓶颈点（缓存压力或 memory bandwidth）。

## B. 访问模式与分层缓存实验（目标：证明三路径设计能降低主机往返、提升吞吐和降低尾延迟）

> 先说明 Baselines：
>
> - Baseline A（传统）：所有元数据驻留 host DRAM（现状实现）；
> - Baseline B（L1-only）：把热点 small fields 固定放 NIC 本地（no CXL）；
> - Proposed（L1 + L2(CXL) + L3）：完整设计。

硬件最好有：支持 RDMA 的 NIC、支持 CXL 的平台或用模拟（把 L2 模拟为“远端内存”并控制延迟/带宽）。如果没有 CXL 实物，强烈建议先用模拟器/延迟注入（在 driver 层模拟 L2 的延迟／带宽特性）来做初步实验。

------

### B1 — 基线延迟与吞吐对比（microbench）

**目的**：证明 Proposed 在平均延迟、尾延迟、CPU 占用与主机 PCIe 流量上优于 Baseline A/B。
 **工作负载**：短消息 RDMA 操作（small msgs 64B/256B/1KB），多 QP 并发（QP count ∈ {1,16,256,4096}），并发线程数 ∈ {1,8,32,128}。
 **步骤**：

1. 在 Baseline A：运行 `ib_send_bw`/`ib_read_bw` 或应用微基准同时记录主机 CPU、PCIe 带宽。
2. 在 Baseline B：把 hot fields 固定到 NIC local memory（simulate），重复。
3. 在 Proposed：启用 L2 (CXL) 路径与 promote/demote。
    **采集**：P50/P95/P99 latency, throughput (ops/s), host CPU %, PCIe MB/s。
    **分析**：绘制多线图（QP count vs latency），展示 Proposed 的优势。

------

### B2 — Hot Inline 路径：低延迟 & 并发竞争测试

**目的**：测量 Hot Inline 在高并发下的实际延迟与 CAS 失败/重试影响。
 **工作负载**：高频访问同一组 hot tokens（模拟热点 QP/CQ），多个 worker 并发发 WR 到同一 token（并发度 ∈ {1,4,16,64,256}）。
 **步骤**：

- 测量：成功 inline updates/sec；CAS 重试次数/秒；P50/P95/P99；CPU 使用率（NIC/host）。
   **分析**：找出 contention 点；如果 CAS 失败率高，记录 fallback 次数，评估整体延迟上升曲线。

------

### B3 — Prefetched Batch：batch size 与聚类策略影响

**目的**：找出最佳 batch size、page cluster 策略和 prefetch window。
 **参数扫表**：batch_size ∈ {8,16,32,64,128,256}；cluster_by_page_size ∈ {4KB,16KB,64KB,256KB}。
 **工作负载**：中等频度对一组 token 的访问（模拟扫描、短流）。
 **步骤**：

- 对每个参数组合运行 60s 测试，测命中率、avg latency、DMA bandwidth。
   **指标**：L2->L1 成功率、avg cost/op、PCIe MB/s。
   **分析**：画热力图（batch_size x cluster_size -> avg latency），找 Pareto 最优点。